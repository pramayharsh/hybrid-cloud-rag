# ğŸ“š Hybrid-Cloud RAG Pipeline

![Python Version](https://img.shields.io/badge/python-3.11%2B-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Framework](https://img.shields.io/badge/framework-LangChain-orange)

A high-performance **Retrieval-Augmented Generation (RAG)** system built to handle private documents with **zero local hardware dependencies**.  
This project utilizes a hybrid-cloud architecture to provide high-performance document intelligence without requiring local GPU resources.

---

## ğŸš€ Architecture (The "Why")

This project was specifically designed to overcome common hardware limitations (DLL errors and SQLite version conflicts) often found in Windows environments by using a **Hybrid-Cloud** approach.

### Tech Stack

- **LLM:** Groq (Llama 3.3-70B) â€” Chosen for ultra-fast inference speeds  
- **Embeddings:** Hugging Face Serverless API (`all-MiniLM-L6-v2`) â€” Moves mathematical vectorization to the cloud, avoiding heavy local PyTorch/ONNX dependencies  
- **Vector Store:** FAISS (Facebook AI Similarity Search) â€” Selected over ChromaDB for better cross-platform stability and lightweight CPU performance  
- **Orchestration:** LangChain (v0.3.x) â€” Using the modern `create_retrieval_chain` architecture  

---

## ğŸ—ï¸ Architecture Diagram

```mermaid
graph LR
    subgraph Ingestion["ğŸ“¥ Document Ingestion"]
        A[ğŸ“„ User Documents<br/>PDF/TXT] --> B[âœ‚ï¸ Text Splitter<br/>1000 chars, 150 overlap]
        B --> C[â˜ï¸ Hugging Face API<br/>all-MiniLM-L6-v2]
        C -->|384-dim vectors| D[(ğŸ—„ï¸ FAISS<br/>Vector Store)]
    end
    
    subgraph Query["ğŸ” Query Processing"]
        E[â“ User Question] --> F[ğŸ” Similarity Search]
        D -.->|retrieve| F
        F -->|Top 3 chunks| G[ğŸ“ Context Builder]
    end
    
    subgraph Generation["âš¡ Answer Generation"]
        G --> H[ğŸ¤– Groq LLM<br/>Llama 3.3-70B]
        H -->|fact-based| I[âœ… Answer]
    end
    
    style D fill:#e1f5ff
    style H fill:#fff4e1
    style I fill:#e8f5e9
```

---

## ğŸ› ï¸ Installation

### 1. Clone the repository
```bash
git clone https://github.com/pramayharsh/hybrid-cloud-rag.git
cd hybrid-rag-pipeline
```

### 2. Create a virtual environment
```bash
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Configure environment variables

Create a `.env` file in the project root:
```env
GROQ_API_KEY=your_key
HUGGINGFACEHUB_API_TOKEN=your_key
```

---

## ğŸ“– How It Works

### Ingestion

Documents (`.pdf` or `.txt`) are loaded and split into:

- **Chunk size:** 1000 characters  
- **Overlap:** 150 characters  

This preserves context across chunks.

### Vectorization

Chunks are sent to the Hugging Face API and converted into **384-dimensional embeddings**.

### Storage

Vectors are stored in a **FAISS index** for high-speed similarity searching.

### Retrieval

When a user asks a question, the **top 3 most relevant chunks** are retrieved.

### Generation

The retrieved context and question are passed to **Llama 3.3 via Groq** to generate a fact-based response.

---

## ğŸ“‚ Project Structure
```text
rag-project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ database.py         # FAISS Vector Store logic
â”‚   â”œâ”€â”€ embeddings_logic.py # HF Cloud Embedding logic
â”‚   â””â”€â”€ llm_logic.py        # Groq LLM configuration
â”œâ”€â”€ data/                   # (Optional) Store your PDFs here
â”œâ”€â”€ .env                    # Secret API Keys (ignored by git)
â”œâ”€â”€ .gitignore              # Files to exclude from version control
â”œâ”€â”€ Dockerfile              # Containerization instructions
â”œâ”€â”€ LICENSE                 # MIT License
â”œâ”€â”€ main.py                 # Interactive CLI entry point
â”œâ”€â”€ requirements.txt        # Project dependencies
â””â”€â”€ README.md               # Project documentation
```

---

## ğŸ› ï¸ Usage Examples (CLI)

### Chatting with your documents

Once initialized, the system enters an interactive loop:
```bash
python main.py
```
```text
--- ğŸ“š Modern RAG System Initializing ---
âœ… System Ready! Ask a question:
You: What is the project budget?
AI: According to the document, the budget is $2.4M for Q1.
```

---

## ğŸ³ Docker Support

### Build the image
```bash
docker build -t hybrid-rag-app .
```

### Run the container

Pass your API keys as environment variables for security:
```bash
docker run -it \
  -e GROQ_API_KEY="your_key" \
  -e HUGGINGFACEHUB_API_TOKEN="your_key" \
  hybrid-rag-app
```

---

## ğŸ§ª Testing the RAG System

To verify that the system uses document data rather than pre-trained knowledge:

- Use the provided `test_facts.txt` file  
- Ask questions about the fictional **"Nebula-X"** project  
- Confirm responses are derived only from the document content

---

## ğŸš€ Key Features

- **Cross-Platform Compatibility** â€” Replaced ChromaDB with FAISS to bypass SQLite version conflicts on Windows/Linux  
- **Serverless Embeddings** â€” Uses Hugging Face Inference API to reduce local memory footprint  
- **Ultra-Low Latency** â€” Powered by Groq's LPU (Language Processing Unit) for near-instant responses  
- **Smart Chunking** â€” Implements `RecursiveCharacterTextSplitter` for optimal context preservation  
- **No GPU Required** â€” Production-ready hybrid-cloud architecture

---

## ğŸ“„ License

Distributed under the MIT License. See the `LICENSE` file for more information.

---

## ğŸ¤ Contributing

Pull requests are welcome. For major changes, please open an issue first to discuss your proposal.

---

## â­ Support

If you find this project useful, consider giving the repository a star â­